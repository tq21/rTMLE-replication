# inference
# eic <- Qstar$Qbar1W-Qstar$Qbar0W-mean(Qstar$Qbar1W-Qstar$Qbar0W)+(A/g1W-(1-A)/g0W)*(Y-Qstar$QbarAW)
# se <- sqrt(var(eic)/length(eic))
# lower <- RiskDiff - 1.96*se
# upper <- RiskDiff + 1.96*se
# var <- var(eic)
# data.frame(Risk1, Risk0, RiskDiff, lower, upper, var, RiskRatio, OddsRatio)
data.frame(Risk1, Risk0, RiskDiff, RiskRatio, OddsRatio)
}
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
Qbar = Qbar,
g1W = g1W,
wt = rep(1, nrow(W)))
rtmle_res
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 1,
Qbar = Qbar,
g1W = g1W,
wt = rep(1, nrow(W)))
rtmle_res
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_res
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
B <- 10
rtmle_psi <- numeric(B)
for (b in 1:B) {
cat("b = ", b, "\n")
idx <- sample(1:nrow(W), nrow(W), replace = TRUE)
rtmle_res <- rtmle(W = W[idx, ],
A = A.outlet[idx],
Y = Y.yr[idx],
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_psi[b] <- rtmle_res$RiskDiff
}
rtmle_psi
qlogis(5)
qlogis(1)
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
B <- 10
rtmle_psi <- numeric(B)
for (b in 1:B) {
cat("b = ", b, "\n")
idx <- sample(1:nrow(W), nrow(W), replace = TRUE)
rtmle_res <- rtmle(W = W[idx, ],
A = A.outlet[idx],
Y = Y.yr[idx],
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_psi[b] <- rtmle_res$RiskDiff
}
rtmle_psi
quantile(rtmle_psi, c(0.025, 0.975))
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
B <- 50
rtmle_psi <- numeric(B)
for (b in 1:B) {
cat("b = ", b, "\n")
idx <- sample(1:nrow(W), nrow(W), replace = TRUE)
rtmle_res <- rtmle(W = W[idx, ],
A = A.outlet[idx],
Y = Y.yr[idx],
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_psi[b] <- rtmle_res$RiskDiff
}
quantile(rtmle_psi, c(0.025, 0.975))
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
B <- 50
rtmle_psi <- numeric(B)
for (b in 1:B) {
cat("b = ", b, "\n")
idx <- sample(1:nrow(W), nrow(W), replace = TRUE)
rtmle_res <- rtmle(W = W[idx, ],
A = A.outlet[idx],
Y = Y.yr[idx],
a = 0,
b = 0.1,
wt = rep(1, nrow(W)))
rtmle_psi[b] <- rtmle_res$RiskDiff
}
quantile(rtmle_psi, c(0.025, 0.975))
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
B <- 50
rtmle_psi <- numeric(B)
for (b in 1:B) {
cat("b = ", b, "\n")
idx <- sample(1:nrow(W), nrow(W), replace = TRUE)
rtmle_res <- rtmle(W = W[idx, ],
A = A.outlet[idx],
Y = Y.yr[idx],
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_psi[b] <- rtmle_res$RiskDiff
}
quantile(rtmle_psi, c(0.025, 0.975))
rtmle_psi
mean(rtmle_psi)
.clip <- function(val, bound) {
pmin(pmax(val, bound[1]), bound[2])
}
#------------------
# tmleLogLike -  calculate the quasi-loglikelihood loss
#		for the TMLE updating step
# Here are using a 2-dimensional covariate for fluctuation bc we want to calculate
#	the marginal risk difference, risk ratio and odds ratio
# input: eps (flucutation parameter), outcome Y, initial estimates of the Qbar(A,W)
#		the clever covariate under no exposure H.0W, the clever covariate under exposure H.1W, weights
# returns: negative quasi-loglikelihood loss
#-----------------
tmleLogLike<- function(eps, Y, Q.AW, H.0W, H.1W, wt=1){
pi<- plogis( qlogis(Q.AW) + eps[1]*H.0W + eps[2]*H.1W)
pi[pi==0] <- .Machine$double.neg.eps		# prevent from taking the log of 0 or 1
pi[pi==1]<- 1-.Machine$double.neg.eps
logLike<- sum( wt*( Y*log(pi)+ (1-Y)*log(1-pi)) )
if(is.na(logLike) | is.nan(logLike) | is.infinite(logLike)){
eps<- c(0,0)	# set epsilon1 and epsilon2 to zero
logLike<- 99
}
return(-logLike)		# log likelihood loss function.
}
#---------------------------
# tmlegrad - corresponding gradient function.
# 	other optimization routines do not require a gradient
#-----------------------------
tmlegrad<- function(eps, Y, Q.AW, H.0W, H.1W, wt=1){
pi<- plogis( qlogis(Q.AW) + eps[1]*H.0W + eps[2]*H.1W)
pi[pi==0] <- .Machine$double.neg.eps
pi[pi==1]<- 1-.Machine$double.neg.eps
resid<- wt*(Y-pi) # weighted residuals
gr<- crossprod( cbind(H.0W, H.1W), resid)
if( sum(is.na(gr))>0 | sum(is.nan(gr))>0 | sum(is.infinite(gr))>0 ){
gr<- c(99, 99)
}
return(-gr)
}
create_custom_glm <- function(var_name) {
force(var_name)
function(Y, X, newX, ...) {
SL.glm(Y = Y, X = X[, var_name, drop = FALSE],
newX = newX[, var_name, drop = FALSE], ...)
}
}
rtmle <- function(W,
A,
Y,
a,
b,
wt) {
#browser()
#-----------------
# Step 0 -  transform outcome
Y.tilde <- (Y-a)/(b-a)
#---------------
# Step 1 -  initial estimation of conditional mean outcome
data_A1 <- cbind(A = 1, W)
data_A0 <- cbind(A = 0, W)
W_names <- names(W)
for (var in W_names) {
assign(paste0("SL.glm.Q.", var), create_custom_glm(var))
assign(paste0("SL.glm.Q.", var, ".A"), create_custom_glm(c("A", var)))
}
Q.SL.lib <- c(paste0("SL.glm.Q.", W_names), paste0("SL.glm.Q.", W_names, ".A"))
Q.SL.lib <- c("SL.mean", Q.SL.lib)
Q.SL <- SuperLearner(Y = Y.tilde, X = cbind(A, W),
family = gaussian(),
SL.library = Q.SL.lib)
QbarAW <- .clip(as.numeric(Q.SL$SL.predict), c(0.01, 0.99))
Qbar1W <- .clip(as.numeric(predict(Q.SL, newdata = data_A1)$pred), c(0.01, 0.99))
Qbar0W <- .clip(as.numeric(predict(Q.SL, newdata = data_A0)$pred), c(0.01, 0.99))
Qbar <- data.frame(QbarAW, Qbar1W, Qbar0W)
# g SL library
for (var in W_names) {
assign(paste0("SL.glm.g.", var), create_custom_glm(var))
}
g.SL.lib <- paste0("SL.glm.g.", W_names)
g.SL.lib <- c("SL.mean", g.SL.lib)
g.SL <- SuperLearner(Y = A, X = W,
family = binomial(),
SL.library = g.SL.lib)
g1W <- .clip(as.numeric(g.SL$SL.predict), c(0.01, 0.99))
#----------------------------
# Step 2 - targeting
g0W <- 1-g1W
# calculate clever covariate -> going to do a 2-dim update here
H.1W <- A/g1W
H.0W <- (1-A)/g0W  # note not negative here.
# it's easier to code a 2-dim update when going after Risk Diff, Risk Ratio, Odds Ratio
# initial parameter estimates are 0
opt.out <- optim(par=c(0,0), fn=tmleLogLike, gr=tmlegrad, Y=Y.tilde,
Q.AW=Qbar$QbarAW, H.0W=H.0W, H.1W=H.1W, wt=wt, method="BFGS",
hessian=F, control=list(maxit=500, trace=F))
eps <- opt.out$par
#print(paste('two-dim epsilon', eps, sep=' '))
# update. still on the transformed scale.
QbarAW <- plogis(qlogis(Qbar$QbarAW)+eps[1]*H.0W + eps[2]*H.1W)
Qbar0W <- plogis(qlogis(Qbar$Qbar0W)+eps[1]/g0W)
Qbar1W <- plogis(qlogis(Qbar$Qbar1W)+eps[2]/g1W)
Q <- data.frame(cbind(QbarAW, Qbar1W, Qbar0W))
colnames(Q)<- c("QbarAW", "Qbar1W", "Qbar0W")
# transform back
Qstar <- Q*(b-a)+a
#------------------
# point estimate
# for CCDesigns, need to average over case-control weighted dist of cov
# see van der Laan 2008
Risk1 <- weighted.mean(Qstar$Qbar1W, w=wt)
Risk0 <- weighted.mean(Qstar$Qbar0W, w=wt)
RiskDiff <- Risk1 - Risk0
RiskRatio <- Risk1/Risk0
OddsRatio <- (Risk1/(1-Risk1))/(Risk0/(1-Risk0))
# inference
eic <- Qstar$Qbar1W-Qstar$Qbar0W-mean(Qstar$Qbar1W-Qstar$Qbar0W)+(A/g1W-(1-A)/g0W)*(Y-Qstar$QbarAW)
se <- sqrt(var(eic)/length(eic))
lower <- RiskDiff - 1.96*se
upper <- RiskDiff + 1.96*se
var <- var(eic)
# data.frame(Risk1, Risk0, RiskDiff, lower, upper, var, RiskRatio, OddsRatio)
data.frame(Risk1, Risk0, RiskDiff, lower, upper, RiskRatio, OddsRatio)
}
idx <- 1:nrow(W)
rtmle_res <- rtmle(W = W[idx, ],
A = A.outlet[idx],
Y = Y.yr[idx],
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_res$lower
rtmle_res$upper
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
rtmle_res
tmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 1,
wt = rep(1, nrow(W)))
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
tmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 1,
wt = rep(1, nrow(W)))
tmle_res
rtmle_res$upper-rtmle_res$lower
tmle_res$upper-tmle_res$lower
library(SuperLearner)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
library(AIPW)
library(SuperLearner)
library(AIPW)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
tmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 1,
wt = rep(1, nrow(W)))
rtmle_res
library(SuperLearner)
library(AIPW)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
tmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 1,
wt = rep(1, nrow(W)))
rtmle_res
.clip <- function(val, bound) {
pmin(pmax(val, bound[1]), bound[2])
}
#------------------
# tmleLogLike -  calculate the quasi-loglikelihood loss
#		for the TMLE updating step
# Here are using a 2-dimensional covariate for fluctuation bc we want to calculate
#	the marginal risk difference, risk ratio and odds ratio
# input: eps (flucutation parameter), outcome Y, initial estimates of the Qbar(A,W)
#		the clever covariate under no exposure H.0W, the clever covariate under exposure H.1W, weights
# returns: negative quasi-loglikelihood loss
#-----------------
tmleLogLike<- function(eps, Y, Q.AW, H.0W, H.1W, wt=1){
pi<- plogis( qlogis(Q.AW) + eps[1]*H.0W + eps[2]*H.1W)
pi[pi==0] <- .Machine$double.neg.eps		# prevent from taking the log of 0 or 1
pi[pi==1]<- 1-.Machine$double.neg.eps
logLike<- sum( wt*( Y*log(pi)+ (1-Y)*log(1-pi)) )
if(is.na(logLike) | is.nan(logLike) | is.infinite(logLike)){
eps<- c(0,0)	# set epsilon1 and epsilon2 to zero
logLike<- 99
}
return(-logLike)		# log likelihood loss function.
}
#---------------------------
# tmlegrad - corresponding gradient function.
# 	other optimization routines do not require a gradient
#-----------------------------
tmlegrad<- function(eps, Y, Q.AW, H.0W, H.1W, wt=1){
pi<- plogis( qlogis(Q.AW) + eps[1]*H.0W + eps[2]*H.1W)
pi[pi==0] <- .Machine$double.neg.eps
pi[pi==1]<- 1-.Machine$double.neg.eps
resid<- wt*(Y-pi) # weighted residuals
gr<- crossprod( cbind(H.0W, H.1W), resid)
if( sum(is.na(gr))>0 | sum(is.nan(gr))>0 | sum(is.infinite(gr))>0 ){
gr<- c(99, 99)
}
return(-gr)
}
create_custom_glm <- function(var_name) {
force(var_name)
function(Y, X, newX, ...) {
SL.glm(Y = Y, X = X[, var_name, drop = FALSE],
newX = newX[, var_name, drop = FALSE], ...)
}
}
rtmle <- function(W,
A,
Y,
a,
b,
wt) {
#browser()
#-----------------
# Step 0 -  transform outcome
Y.tilde <- (Y-a)/(b-a)
#---------------
# Step 1 -  initial estimation of conditional mean outcome
data_A1 <- cbind(A = 1, W)
data_A0 <- cbind(A = 0, W)
W_names <- names(W)
for (var in W_names) {
assign(paste0("SL.glm.Q.", var), create_custom_glm(var))
assign(paste0("SL.glm.Q.", var, ".A"), create_custom_glm(c("A", var)))
}
Q.SL.lib <- c(paste0("SL.glm.Q.", W_names), paste0("SL.glm.Q.", W_names, ".A"))
Q.SL.lib <- c("SL.mean", Q.SL.lib)
Q.SL <- SuperLearner(Y = Y.tilde, X = cbind(A, W),
family = gaussian(),
SL.library = Q.SL.lib)
QbarAW <- .clip(as.numeric(Q.SL$SL.predict), c(0.01, 0.99))
Qbar1W <- .clip(as.numeric(predict(Q.SL, newdata = data_A1)$pred), c(0.01, 0.99))
Qbar0W <- .clip(as.numeric(predict(Q.SL, newdata = data_A0)$pred), c(0.01, 0.99))
Qbar <- data.frame(QbarAW, Qbar1W, Qbar0W)
# g SL library
for (var in W_names) {
assign(paste0("SL.glm.g.", var), create_custom_glm(var))
}
g.SL.lib <- paste0("SL.glm.g.", W_names)
g.SL.lib <- c("SL.mean", g.SL.lib)
g.SL <- SuperLearner(Y = A, X = W,
family = binomial(),
SL.library = g.SL.lib)
g1W <- .clip(as.numeric(g.SL$SL.predict), c(0.01, 0.99))
#----------------------------
# Step 2 - targeting
g0W <- 1-g1W
# calculate clever covariate -> going to do a 2-dim update here
H.1W <- A/g1W
H.0W <- (1-A)/g0W  # note not negative here.
# it's easier to code a 2-dim update when going after Risk Diff, Risk Ratio, Odds Ratio
# initial parameter estimates are 0
opt.out <- optim(par=c(0,0), fn=tmleLogLike, gr=tmlegrad, Y=Y.tilde,
Q.AW=Qbar$QbarAW, H.0W=H.0W, H.1W=H.1W, wt=wt, method="BFGS",
hessian=F, control=list(maxit=500, trace=F))
eps <- opt.out$par
#print(paste('two-dim epsilon', eps, sep=' '))
# update. still on the transformed scale.
QbarAW <- plogis(qlogis(Qbar$QbarAW)+eps[1]*H.0W + eps[2]*H.1W)
Qbar0W <- plogis(qlogis(Qbar$Qbar0W)+eps[1]/g0W)
Qbar1W <- plogis(qlogis(Qbar$Qbar1W)+eps[2]/g1W)
Q <- data.frame(cbind(QbarAW, Qbar1W, Qbar0W))
colnames(Q)<- c("QbarAW", "Qbar1W", "Qbar0W")
# transform back
Qstar <- Q*(b-a)+a
#------------------
# point estimate
# for CCDesigns, need to average over case-control weighted dist of cov
# see van der Laan 2008
Risk1 <- weighted.mean(Qstar$Qbar1W, w=wt)
Risk0 <- weighted.mean(Qstar$Qbar0W, w=wt)
RiskDiff <- Risk1 - Risk0
RiskRatio <- Risk1/Risk0
OddsRatio <- (Risk1/(1-Risk1))/(Risk0/(1-Risk0))
# inference
eic <- Qstar$Qbar1W-Qstar$Qbar0W-mean(Qstar$Qbar1W-Qstar$Qbar0W)+(A/g1W-(1-A)/g0W)*(Y-Qstar$QbarAW)
se <- sqrt(var(eic)/length(eic))
lower <- RiskDiff - 1.96*se
upper <- RiskDiff + 1.96*se
var <- var(eic)
# AIPW
aipw_nuisance <- AIPW_nuis$new(Y = Y,
A = A,
mu0 = Qbar0W*(b-a)+a,
mu1 = Qbar1W*(b-a)+a,
raw_p_score = g1W*A+(1-A)*g0W,
verbose = FALSE)$summary()
browser()
data.frame(Risk1,
Risk0,
RiskDiff,
lower,
upper,
var,
AIPW_RiskDiff = aipw_nuisance$result["Risk Difference", "Estimate"],
AIPW_lower = aipw_nuisance$result["Risk Difference", "95% LCL"],
AIPW_upper = aipw_nuisance$result["Risk Difference", "95% UCL"])
}
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
aipw_nuisance$result
0.005069373^2
library(SuperLearner)
library(AIPW)
load("out/NYC.clustered.Rdata")
source("utils.R")
set.seed(123)
rtmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 0.085,
wt = rep(1, nrow(W)))
tmle_res <- rtmle(W = W,
A = A.outlet,
Y = Y.yr,
a = 0,
b = 1,
wt = rep(1, nrow(W)))
rtmle_res
tmle_res
